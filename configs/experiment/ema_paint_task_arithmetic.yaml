# @package _global_
experiment:
  type: 'class_incremental'

  # Turn on open vocabulary classification style.
  training: 'contrastive'

  backbone:
    name: openclip_vit_b16
    cache_dir: './cache'
    head: default
    pretrained: True
    freeze_head: False
    freeze_features: False
    half_precision: True

  dataset:
    sequence: 'stream_construction/random_sequence_1.json'
    path: './data'
    pretraining_data_path: './data/laion400m/shards'
    preload: False
    create_resized_variant_if_possible: True
    num_workers: 8
    resize: 224
    img_size: 224
    train_transforms: [
      'RandomResizedCrop', 'ToTensor', 'Normalize'
    ]
    test_transforms: [
      'Resize','CenterCrop','ToTensor','Normalize'
    ]

  evaluation:
    batch_size: 512
    additional_datasets: ['caltech101','caltech256','cars196','cifar10','domainnet_quickdraw','eurosat','fashionmnist','food101','gtsrb','imagenet','imagenet_a','imagenet_d','imagenet_r','imagenet_s','imagenet_v2','mnist','monkeys10','oxford_pets','stl10','svhn','mscoco','flickr30k']
    validate_on_subset: 0.1

  # If methods utilize a buffer:
  buffer:
    size: 500

  task:
    num: 20
    n_samples: 727040
    batch_size: 512
    eval_every_n_samples: 727040
    data_mixture:
      pretraining: 0.33
      update: 0.34
      buffer: 0.33

  optimizer:
    name: 'adamw'
    lr: 0.00001
    scaled_learning_rate: False
    loss: clip
    label_smoothing: 0
    weight_decay: 0.2
    clip_grad_norm: 1
    clip_temperature: 'learnable'

  scheduler:
    name: 'cosine_with_linear_warmup'
    cosine_lr_mul: 0.001
    warmup_perc: 0.1

log:
  project: continualfomo_finetune
  group: test
  name: ema_paint_task_arithmetic
  folder: ./checkpoints
  checkpoint: False

continual:
  method: ema_paint
  ema_paint:
    backbone_merge:
      method: 'task_arithmetic'
      apply_lines: False
      task_arithmetic:
        scaling_factor: 0.1
    head_merge:
      method: 'task_arithmetic'
      apply_lines: False
      task_arithmetic:
        scaling_factor: 0.1